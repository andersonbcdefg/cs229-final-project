{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test sets of tweets\n",
    "Before feature extraction, we do the train-dev-test split, so that these datasets are constant across feature extraction methods. For binary classification, the positive examples come from the Russian troll tweets dataset. Negative examples are a combination of a sentiment dataset, and a dataset of tweets from Republican and Democratic politicians. (We want to make sure that there are negative examples that still have a \"political\" orientation, since our goal is to tell troll tweets from real tweets, rather than political from non-political.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_examples = pd.read_csv(\"data/preprocessed-text/preprocessed-troll-tweets.csv\")\n",
    "neg_sentiment_examples = pd.read_csv(\"data/preprocessed-text/sentiment-preprocessed.csv\",\n",
    "                                    encoding=\"latin\").rename(columns={\"account_type\":\"account_category\"})\n",
    "neg_political_examples = pd.read_csv(\n",
    "    \"data/preprocessed-text/big-political-preprocessed.csv\").rename(columns={\n",
    "    \"account_type\":\"account_category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970780, 1600498, 1243370)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_examples), len(neg_sentiment_examples), len(neg_political_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'account_category', 'troll'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sentiment_examples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 229\n",
    "combined = pd.concat([\n",
    "    pos_examples,\n",
    "    neg_sentiment_examples.sample(n=1000000, random_state=random_state),\n",
    "    neg_political_examples.sample(n=1000000, random_state=random_state)\n",
    "]).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>account_category</th>\n",
       "      <th>troll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@BilgeEbiri really nails why many villains in...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton, Trump lead 2016 delegate race https:/...</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @the_intercept: How our reporter @JuanMThom...</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cruz, Colbert debate Reagan, gay marriage  #en...</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'@420omnivore @leyalouisee May be this girl wi...</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>More to Jamaica than 'anti-gay Gestapos': Man ...</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Darkness cannot drive out darkness; only light...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm thrilled to be here @ #CBCFALC2012 hosting...</td>\n",
       "      <td>NotTroll</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nearly all men can stand adversity, but if you...</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‘Reprehensible’ fondling of 7-year-old girl by...</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @margmiley: @sethmoulton @GOP Grateful that...</td>\n",
       "      <td>NotTroll</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Over 23 percent of Florida’s workforce will be...</td>\n",
       "      <td>NotTroll</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#mar RT BelindaBee13: You Keith are crazy, fou...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Threat forces flight to evacuate at JFK Airpor...</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DEMENTED House Democrat Vows to “ELIMINATE” Tr...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is very happy</td>\n",
       "      <td>NotTroll</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Police: Man Smoking Heroin In Car Tried To Run...</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wells Fargo cheated customers &amp;amp; until it l...</td>\n",
       "      <td>NotTroll</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The financial crisis begin ten years ago today...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@aristofontes we all know very well that our s...</td>\n",
       "      <td>NotTroll</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content account_category  troll\n",
       "0   .@BilgeEbiri really nails why many villains in...       RightTroll   True\n",
       "1   Clinton, Trump lead 2016 delegate race https:/...         NewsFeed   True\n",
       "2   RT @the_intercept: How our reporter @JuanMThom...        LeftTroll   True\n",
       "3   Cruz, Colbert debate Reagan, gay marriage  #en...         NewsFeed   True\n",
       "4   '@420omnivore @leyalouisee May be this girl wi...        LeftTroll   True\n",
       "5   More to Jamaica than 'anti-gay Gestapos': Man ...         NewsFeed   True\n",
       "6   Darkness cannot drive out darkness; only light...       RightTroll   True\n",
       "7   I'm thrilled to be here @ #CBCFALC2012 hosting...         NotTroll  False\n",
       "8   Nearly all men can stand adversity, but if you...        LeftTroll   True\n",
       "9   ‘Reprehensible’ fondling of 7-year-old girl by...         NewsFeed   True\n",
       "10  RT @margmiley: @sethmoulton @GOP Grateful that...         NotTroll  False\n",
       "11  Over 23 percent of Florida’s workforce will be...         NotTroll  False\n",
       "12  #mar RT BelindaBee13: You Keith are crazy, fou...       RightTroll   True\n",
       "13  Threat forces flight to evacuate at JFK Airpor...         NewsFeed   True\n",
       "14  DEMENTED House Democrat Vows to “ELIMINATE” Tr...       RightTroll   True\n",
       "15                                      is very happy         NotTroll  False\n",
       "16  Police: Man Smoking Heroin In Car Tried To Run...         NewsFeed   True\n",
       "17  Wells Fargo cheated customers &amp; until it l...         NotTroll  False\n",
       "18  The financial crisis begin ten years ago today...       RightTroll   True\n",
       "19  @aristofontes we all know very well that our s...         NotTroll  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)\n",
    "combined.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"troll\": combined.groupby(\"account_category\").count(), \n",
    "    \"categories\": combined.groupby(\"troll\").count()\n",
    "}\n",
    "pickle.dump(metadata, open(\"data/preprocessed-text/combined-metadata.pickle\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff, dev_cutoff = int(len(combined) * 0.7), int(len(combined) * 0.85)\n",
    "train_tweets = combined.iloc[:train_cutoff,:]\n",
    "dev_tweets = combined.iloc[train_cutoff:dev_cutoff,:]\n",
    "test_tweets = combined.iloc[dev_cutoff:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.to_csv(\"data/preprocessed-text/train_tweets.csv\", index=False)\n",
    "dev_tweets.to_csv(\"data/preprocessed-text/dev_tweets.csv\", index=False)\n",
    "test_tweets.to_csv(\"data/preprocessed-text/test_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT features\n",
    "Take these raw tweets for train, dev, and test sets and use pretrained BERT to create features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = pd.read_csv(\"data/preprocessed-text/train_tweets.csv\",\n",
    "                          dtype={'content':'string', 'account_category':'string', 'troll':'boolean'}).dropna()\n",
    "dev_tweets = pd.read_csv(\"data/preprocessed-text/dev_tweets.csv\",\n",
    "                          dtype={'content':'string', 'account_category':'string', 'troll':'boolean'}).dropna()\n",
    "test_tweets = pd.read_csv(\"data/preprocessed-text/test_tweets.csv\",\n",
    "                          dtype={'content':'string', 'account_category':'string', 'troll':'boolean'}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# feature_extractor = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Pandas Series of tweets.\n",
    "# Output: DataFrame of features where each column is a feature,\n",
    "#         and each row is a tweet.\n",
    "def bert_featurize(tweets, model, tokenizer):\n",
    "    encoded = tweets.apply(lambda t: tokenizer.encode(t))\n",
    "    max_len = np.max([len(t) for t in encoded.tolist()])\n",
    "    padded = encoded.apply(lambda t: np.array(t + [0] * (max_len - len(t))))\n",
    "    model_input = torch.tensor(np.vstack(padded.values))\n",
    "    attention_mask = torch.tensor(np.where(model_input == 0, 0, 1))\n",
    "    with torch.no_grad():\n",
    "        output = model(model_input, attention_mask=attention_mask)\n",
    "    return pd.DataFrame(output[0][:, 0, :].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Pandas DataFrame including \"content\" column for tweets.\n",
    "# Output: Same DataFrame with BERT features added in new columns.\n",
    "def bert_featurize_df(df, model, tokenizer, batch_size, outfile):\n",
    "    for idx in tqdm(range(0, len(df), batch_size)):\n",
    "        chunk = df.iloc[idx:idx + batch_size, :]\n",
    "        bert_features = bert_featurize(chunk[\"content\"], model, tokenizer)\n",
    "        combined = chunk.reset_index(drop=True).join(bert_features.reset_index(drop=True))\n",
    "        combined.to_csv(outfile, mode='a', index=False, header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48c2ec3bb7547909c75e2ad80f50af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# simple test\n",
    "bert_featurize_df(train_tweets[:200], model, tokenizer, batch_size=10, outfile=\"data/transformer-binary/tiny.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller datasets, taking a subsample of each of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_featurize_df(train_tweets[:100000], model, tokenizer, batch_size=50, \n",
    "                  outfile=\"data/transformer-binary/bert_train_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_featurize_df(dev_tweets[:15000], model, tokenizer, batch_size=50,\n",
    "                 outfile=\"data/transformer-binary/dev_bert_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_featurize_df(test_tweets[:15000], model, tokenizer, batch_size=50,\n",
    "                 outfile=\"data/transformer-binary/test_bert_small.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger datasets, using the entirety of each split. (Takes a *long* time to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cada64e2894dcab6638fed8b41d2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=55060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_featurize_df(train_tweets.dropna()[1403050:], model, tokenizer, batch_size=25,\n",
    "                 outfile=\"data/transformer-binary/train_bert_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3380af55cd384d4c9c5cff10a3a77e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11913.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_featurize_df(dev_tweets, model, tokenizer, batch_size=50,\n",
    "                                  outfile=\"data/transformer-binary/dev_bert_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499ec561edfc4d05a16569dd09750322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5957.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_featurize_df(test_tweets, model, tokenizer, batch_size=100,\n",
    "                  outfile=\"data/transformer-binary/test_bert_large.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Bag of Words\" Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = pd.read_csv(\"data/preprocessed-text/train_tweets.csv\",\n",
    "                          dtype={'content':'string', 'account_category':'string', 'troll':'boolean'}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeaturizer():\n",
    "    def __init__(self, load_path=None):\n",
    "        self.word_counts = None\n",
    "        self.vocab = None\n",
    "        self.SVD = None\n",
    "        if load_path is not None:\n",
    "            self.word_counts, self.vocab, self.SVD = pickle.load(open(load_path, \"rb\"))\n",
    "        \n",
    "    # Input: List of Tweets\n",
    "    def count_words(self, tweets):\n",
    "        # Use training data to construct vocabulary\n",
    "        self.word_counts = defaultdict(int)\n",
    "        for tweet in tqdm(tweets):\n",
    "            words = re.sub(r'[^\\w\\s#]', '', tweet).lower().strip().split(\" \")\n",
    "            for word in words:\n",
    "                self.word_counts[word] += 1\n",
    "    \n",
    "    # Create vocab, filtering out words that occur \n",
    "    # more times than min_occ, less than max_occ\n",
    "    def create_vocab(self, min_occ=0, max_occ=float(\"inf\")):\n",
    "        # Filter out words that occur more than 0.75x the number of tweets, or less than 100 times.\n",
    "        filtered = {k:v for k, v in self.word_counts.items() if v >= min_occ and v < max_occ}\n",
    "        self.vocab = {word: i for i, word in enumerate(filtered)}\n",
    "        print(\"Vocab Length:\", len(self.vocab))\n",
    "        \n",
    "    def tweet_to_arr(self, tweet):\n",
    "        words = re.sub(r'[^\\w\\s#]', '', tweet).lower().strip().split(\" \")\n",
    "        idxs = [self.vocab[word] for word in words if word in self.vocab]\n",
    "        arr = np.zeros((len(self.vocab),))\n",
    "        arr[tuple([idxs])] = 1\n",
    "        return arr\n",
    "    \n",
    "    # For efficiency, provide a sample of the dataframe, not the entire thing.\n",
    "    def fit_svd(self, n_components, df):\n",
    "        self.SVD = TruncatedSVD(n_components = n_components)\n",
    "        arrs = df.content.apply(lambda tweet: self.tweet_to_arr(tweet))\n",
    "        features = np.vstack(arrs.values)\n",
    "        print(features.shape)\n",
    "        self.SVD.fit(features)\n",
    "    \n",
    "    def bag_of_words_featurize(self, df, batch_size, outfile, svd=False):\n",
    "        for idx in tqdm(range(0, len(df), batch_size)):\n",
    "            chunk = df.iloc[idx:idx + batch_size, :]\n",
    "            arrs = chunk.content.apply(lambda tweet: self.tweet_to_arr(tweet))\n",
    "            features = np.vstack(arrs.values)\n",
    "            if svd:\n",
    "                features = self.SVD.transform(features)\n",
    "            combined = chunk.reset_index(drop=True).join(pd.DataFrame(features).reset_index(drop=True))\n",
    "            combined.to_csv(outfile, mode='a', index=False, header=(idx == 0)) \n",
    "    \n",
    "    def save_model(self, outfile):\n",
    "        data = [self.word_counts, self.vocab, self.SVD]\n",
    "        pickle.dump(data, open(outfile, \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TextFeaturizer(\"text-featurizer.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3808)\n"
     ]
    }
   ],
   "source": [
    "sample = train_tweets.sample(200000)\n",
    "tf.fit_svd(500, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([tf.word_counts, tf.vocab, tf.SVD], open(\"text-featurizer.pickle\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a836390556244f109705fbffd46b6318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=556.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.bag_of_words_featurize(train_tweets, 5000, \"./data/bag-of-words/train_bow_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea36573b975b4a95a4241bc065dad586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_tweets = pd.read_csv(\"data/preprocessed-text/dev_tweets.csv\",\n",
    "                          dtype={'content':'string', 'account_category':'string', 'troll':'boolean'}).dropna()\n",
    "tf.bag_of_words_featurize(dev_tweets, 5000, \"./data/bag-of-words/dev_bow_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c1f5171b114ae48ab539e688d0abbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_tweets = pd.read_csv(\"data/preprocessed-text/test_tweets.csv\",\n",
    "                          dtype={'content':'string', 'account_category':'string', 'troll':'boolean'}).dropna()\n",
    "tf.bag_of_words_featurize(test_tweets, 5000, \"./data/bag-of-words/test_bow_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4586e510a1464f6f8ee00945bd665141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=556.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.bag_of_words_featurize(train_tweets, 5000, \"./data/bag-of-words-binary/train_bow_large.csv\", svd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f59460735a4f7c9ba30e6d30e6ada7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.bag_of_words_featurize(dev_tweets, 5000, \"./data/bag-of-words-binary/dev_bow_large.csv\", svd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700dc100a9ff4ac6bd408d5b230fddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.bag_of_words_featurize(test_tweets, 5000, \"./data/bag-of-words-binary/test_bow_large.csv\", svd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
